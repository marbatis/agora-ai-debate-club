# AGORA â€” AI Debate Club: A Multi-Agent System for Structured, Evidence-Based Debates

**Author:** Marcelo Silveira  
**Track:** Freestyle  
**GitHub:** https://github.com/marbatis/agora-ai-debate-club  
**Course:** 5-Day AI Agents Intensive with Google  
**ðŸŽ¥ Demo Video:** https://youtu.be/jfo-z-_Le2I

---

## The Pitch: Problem, Solution, Value

### The Problem: Chaos in Digital Discourse

Online debates suffer from critical issues: lack of structure (participants talk past each other), absence of evidence (unchecked claims), no objective evaluation, and echo chambers reinforcing existing beliefs. This makes productive discussions on important topics like AI regulation or climate policy nearly impossible.

### The Solution: AGORA AI Debate Club

AGORA (Automated Gathering of Rational Arguments) is a multi-agent debate system addressing these challenges:

- **Two Debater Agents** (Pro and Con) - Each maintains distinct perspectives with evidence-backed reasoning
- **One Judge Agent** - Evaluates arguments using structured rubrics (logic, factuality, persuasion)
- **Fact-Checking Tools** - Google Custom Search integration for evidence verification
- **Transparent Scoring** - All evaluations include detailed rationale

The system transforms chaotic debates into structured, turn-based exchanges where arguments are clearly presented and scoring is transparent.

### The Value: Why This Matters

AGORA demonstrates important capabilities for **education** (critical thinking demonstrations), **decision-making** (evaluating strategic options), **research** (studying argumentation patterns), and **AI development** (showcasing multi-agent problem-solving).

---

## Why Agents? The Multi-Agent Advantage

A single LLM cannot effectively debate itselfâ€”it produces weak strawman arguments, inconsistent perspectives, and lacks genuine opposition. The multi-agent architecture creates authentic separation:

- **Debater A (Pro)** has independent context, memory, and instructions arguing FOR
- **Debater B (Con)** independently argues AGAINST with separate reasoning
- **Judge** operates impartially with no stake in either position

Each debater maintains consistent stance, develops independent arguments, cites different sources, and produces genuine rebuttals addressing specific claims.

---

## Architecture: How AGORA Works

### System Design

Sequential multi-agent pattern with centralized orchestration:

```
User Topic â†’ Debate Manager â†’ [Debater A âŸ· Debater B] â†’ Judge â†’ Results
                                      â†“
                              Fact-Check Tools
```

**Key Components:**

1. **Debate Manager** - Orchestrates multi-round debates, manages turn-taking, maintains transcript, determines winner
2. **Debater Agents** - Powered by Gemini 2.5 Flash with distinct prompts, conversation history, fact-checking tools, citation management
3. **Judge Agent** - LLM-as-Judge with rubric (Logic 0-10, Factuality 0-10, Persuasion 0-10), returns JSON with scores + rationale
4. **Fact-Check Tool** - MCP-compatible, Google Custom Search integration, returns top 3 citations
5. **FastAPI Service** - RESTful API (/healthz, /demo, /debate), CORS enabled, structured responses

### Data Flow

1. User submits topic â†’ "Should artificial intelligence be regulated?"
2. Manager starts Round 1: Both debaters generate arguments with optional fact-checking, create rebuttals, Judge scores the round
3. Rounds 2-3: Agents access conversation history, build on previous points, Judge evaluates cumulative performance
4. Final Results: Aggregate scores, winner determined, full transcript returned

---

## Implementation: Technical Deep Dive

### 5 Core AI Agent Features (3+ Required):

**1. âœ… Multi-Agent System**  
- Sequential orchestration with specialized roles (Debater A, B, Judge)
- Coordinated via centralized Debate Manager
- Code: `src/services/debate.py`, `src/agents/`

**2. âœ… Tools Integration**  
- Fact-checking via Google Custom Search (MCP-compatible)
- Graceful degradation without API keys
- Code: `src/tools/factcheck.py`

**3. âœ… Sessions & Memory**  
- Round transcript stored in debate context
- Each agent accesses conversation history
- Claims indexed per side for reference
- Code: `src/services/debate.py`

**4. âœ… Observability**  
- Structured logging via Loguru
- Captures: agent generation steps, arguments, citations, judge rationale, API requests
- Code: Throughout codebase with `logger.debug()` and `logger.info()`

**5. âœ… Agent Evaluation**  
- LLM-as-Judge with three-dimensional rubric
- JSON-formatted output with rationale
- Enhanced parsing for markdown responses
- Code: `src/agents/judge.py`, `src/evaluation/rubric.py`

### Code Quality

- **Clean Interfaces:** Simple, clear agent methods
- **Type Safety:** Pydantic models, type hints throughout
- **Testing:** 3 smoke tests, mock mode, FastAPI TestClient integration
- **Documentation:** Comprehensive README, inline comments, auto-generated API docs

---

## Technical Challenges & Solutions

**Challenge 1: Judge Returning Invalid JSON**  
*Solution:* Enhanced parser stripping markdown code blocks, extracting JSON from mixed text

**Challenge 2: Model Compatibility**  
*Solution:* Updated to gemini-2.5-flash, made model configurable via environment

**Challenge 3: Browser CORS Errors**  
*Solution:* Added FastAPI CORS middleware for local development

**Challenge 4: Long Debate Times**  
*Solution:* Loading messages with time estimates, 3-minute timeout, default 1 round

---

## Results: What We Learned

### Quantitative Results
- Judge scores averaged 7-8/10 across rubric dimensions
- Arguments: 150-250 words with 1-3 citations when enabled
- Processing: 1 round (~30-60s), 3 rounds (~120-180s)

### What Worked Well
- Distinct perspectives maintained across rounds
- Clear judge rationale referencing specific claims
- Coherent turn-based flow
- Quality citations when enabled

### Limitations
- Factual verification limited by search API quality
- Arguments sometimes rely on common talking points
- Rebuttals could address opponent claims more directly
- Rubric may miss nuances like emotional appeal or fallacy detection

### Example Output
**Topic:** "Should artificial intelligence be regulated?"  
**Debater A (Pro):** AI safety, existential risks, regulatory precedents (2 sources)  
**Debater B (Con):** Innovation stifling, regulatory capture, competitiveness (2 sources)  
**Judge Decision:** B won 7.6 to 7.2 - stronger specific examples and logical connections

---

## Journey: Building AGORA

**Week 1:** Multi-agent architecture, Gemini integration, basic debate flow  
**Week 2:** Fact-checking tool, judge rubric, FastAPI service  
**Week 3:** Web interface, enhanced parsing, testing, documentation

**Key Learnings:**
1. Agent separation of concerns is critical
2. Prompts must explicitly constrain behavior ("ONLY JSON")
3. Robust parsing essential for unpredictable LLM outputs
4. Loading indicators crucial for managing user expectations
5. Clear documentation makes the difference between prototype and usable tool

---

## Future Enhancements

**Short-term:**
- Deploy to Google Cloud Run
- Add A2A Protocol for inter-agent messaging
- Implement deeper fact-verification using RAG

**Medium-term:**
- Enhanced rubrics detecting logical fallacies
- Agent personas (Academic vs Politician styles)
- Multi-modal debates (text + images)
- Human-in-the-loop moderation

**Long-term Vision:**
- Public platform for human vs AI debates
- Educational dashboard with argument quality metrics
- Research tool for studying persuasion
- Integration with knowledge bases for domain-specific debates

---

## Conclusion

AGORA demonstrates that multi-agent systems create structured, evidence-based debates superior to single-agent approaches. By separating perspectives and using objective evaluation, we've built a system exploring topics thoroughly and transparently.

The project showcases practical applications from the Google AI Agents Intensive: multi-agent orchestration, tool integration, memory management, observability, and evaluation. It's production-ready with clean API, comprehensive testing, and user-friendly interface.

**Try it yourself:** Clone the repo, add your Gemini API key, and watch AI agents debate any topic you choose!

---

**Word Count:** 1,285 / 1,500

**Technologies:** Python 3.9+, Google Gemini 2.5 Flash, FastAPI, Pydantic, Pytest

**Acknowledgments:** Google AI Agents Intensive Course, Kaggle Community, Google Gemini Team
